{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"video_emotion_rgb.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"p2jRbyC8mbM0","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvpmW8-jl6pt","colab_type":"code","colab":{}},"source":["import os\n","#os.chdir(\"content\")\n","os.chdir(\"drive\")\n","os.chdir(\"My Drive\")\n","os.chdir(\"Facial Emotion Recognition\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KYx25XCZ96z","colab_type":"code","colab":{}},"source":["!pip install tensorflow==1.12.0\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4gIxvovnfMF","colab_type":"code","colab":{}},"source":["from statistics import mode\n","\n","import cv2\n","from keras.models import load_model\n","import numpy as np\n","import PIL\n","from utils.datasets import get_labels\n","from utils.inference import detect_faces\n","from utils.inference import draw_text\n","from utils.inference import draw_bounding_box\n","from utils.inference import apply_offsets\n","from utils.inference import load_detection_model\n","from utils.preprocessor import preprocess_input\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1IY-y_AofOI","colab_type":"code","colab":{}},"source":["!pwd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFMxMkaosNy3","colab_type":"code","colab":{}},"source":["detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n","emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n","gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n","gender_labels = get_labels('imdb')\n","emotion_labels = get_labels('fer2013')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2s-qd40sdw6","colab_type":"code","colab":{}},"source":["frame_window = 10\n","emotion_offsets = (20, 40)\n","gender_offsets = (30, 60)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIikK_dVsgx_","colab_type":"code","colab":{}},"source":["face_detection = load_detection_model(detection_model_path)\n","gender_classifier = load_model(gender_model_path, compile=False)\n","emotion_classifier = load_model(emotion_model_path, compile=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"muvba-C3sjov","colab_type":"code","colab":{}},"source":["emotion_target_size = emotion_classifier.input_shape[1:3]\n","gender_target_size = gender_classifier.input_shape[1:3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7S9AkzAsn2Q","colab_type":"code","colab":{}},"source":["print(emotion_target_size)\n","print(gender_target_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yl5t5nVmsqPk","colab_type":"code","colab":{}},"source":["!pip install face_recognition"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUEXZLnQs2Sj","colab_type":"code","colab":{}},"source":["import face_recognition"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uw5TdoNitRy5","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import sys\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VFdEilet9Yn","colab_type":"code","colab":{}},"source":["sample_video_path = \"../../Diego_Luna_interview2.mp4\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oGOAkPhuRog","colab_type":"code","colab":{}},"source":["# starting lists for calculating modes\n","#emotion_window = []\n","\n","# starting video streaming\n","#cv2.namedWindow('window_frame')\n","video_capture = cv2.VideoCapture(sample_video_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gkKTLBf0bmu","colab_type":"code","colab":{}},"source":["tot = 0\n","while True:\n","    ret, bgr_image = video_capture.read()\n","    if ret == False:\n","        break\n","    tot += 1\n","print(tot)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41neddTXw4RD","colab_type":"code","colab":{}},"source":["face_data = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryL9NDR6etQS","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ap9USv2bvPwH","colab_type":"code","cellView":"form","colab":{}},"source":["#@title process1 { form-width: \"20%\" }\n","tot_frames = 0\n","face_index = 0\n","while True:\n","    ret, bgr_image = video_capture.read()\n","    \n","    if ret == False:\n","        break\n","    \n","    \n","    frame_time =  video_capture.get(cv2.CAP_PROP_POS_MSEC)\n","    \n","    #if tot_frames > 1000:\n","    #    break\n","    \n","    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n","    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n","    faces = detect_faces(face_detection, gray_image)\n","    tot_frames += 1\n","    #print(\"gray_image shape: \",gray_image.shape)\n","\n","    for face_coordinates in faces:\n","        \n","        face_index += 1\n","        \n","        face_dict = {}\n","        \n","        #face_dict[\"rgb_image\"] = rgb_image\n","        face_dict[\"bb\"] = face_coordinates\n","        face_dict[\"face_index\"] = face_index\n","        face_dict[\"frame_number\"] = tot_frames\n","        \n","        a,b,c,d = face_coordinates\n","        \n","        \n","        encodings = face_recognition.face_encodings(rgb_image, [(a,b,a+c, b+d)])\n","\n","        #print(\"encoding shape: \", encodings[0].shape)\n","        face_dict[\"encodings\"] = encodings[0]\n","        \n","        \n","        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n","        \n","        gray_face = gray_image[y1:y2, x1:x2]\n","        try:\n","            gray_face = cv2.resize(gray_face, (emotion_target_size))\n","        except:\n","            continue\n","\n","        gray_face = preprocess_input(gray_face, True)\n","        gray_face = np.expand_dims(gray_face, 0)\n","        gray_face = np.expand_dims(gray_face, -1)\n","        emotion_prediction = emotion_classifier.predict(gray_face)\n","        emotion_probability = np.max(emotion_prediction)\n","        emotion_label_arg = np.argmax(emotion_prediction)\n","        emotion_text = emotion_labels[emotion_label_arg]\n","        \n","        \n","        \n","        face_dict[\"emotion_text\"] = emotion_text\n","        face_dict[\"frame_time\"] = frame_time\n","        \n","        face_data.append(face_dict)\n","        print(\"face_data len: %d\"%len(face_data))\n","        if tot_frames % 1 == 0:\n","            print(\"Face_index: \", face_index)\n","            print(\"Frame time: \", frame_time)\n","            print(\"Face emotion: \", emotion_text)\n","            print(\"Frame_number: \", tot_frames)\n","        \n","            #rgb_image_copy = rgb_image.copy()\n","        \n","            #cv2.rectangle(rgb_image_copy, (a, b), (a+c, b+d), (255, 0, 0), 2)\n","            #cropped_face = rgb_image_copy[b:b+d, a:a+c,:]\n","            #print(cropped_face.shape)\n","            #plt.axis(\"off\")\n","            #plt.imshow(cropped_face)\n","            #plt.show()\n","        \n","        '''\n","        emotion_window.append(emotion_text)\n","\n","        if len(emotion_window) > frame_window:\n","            emotion_window.pop(0)\n","        try:\n","            emotion_mode = mode(emotion_window)\n","        except:\n","            continue\n","\n","       \n","        if emotion_text == 'angry':\n","            color = emotion_probability * np.asarray((255, 0, 0))\n","        elif emotion_text == 'sad':\n","            color = emotion_probability * np.asarray((0, 0, 255))\n","        elif emotion_text == 'happy':\n","            color = emotion_probability * np.asarray((255, 255, 0))\n","        elif emotion_text == 'surprise':\n","            color = emotion_probability * np.asarray((0, 255, 255))\n","        else:\n","            color = emotion_probability * np.asarray((0, 255, 0))\n","\n","        color = color.astype(int)\n","        color = color.tolist()\n","\n","        draw_bounding_box(face_coordinates, rgb_image, color)\n","        draw_text(face_coordinates, rgb_image, emotion_mode,\n","                  color, 0, -45, 1, 1)\n","        '''\n","        \n","        \n","        \n","        \n","    #bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n","    #cv2.imshow('window_frame', bgr_image)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","        \n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"50yQGI1PDHnz","colab_type":"code","cellView":"form","colab":{}},"source":["#@title process2 { form-width: \"20%\" }\n","tot_frames = 0\n","face_index = 0\n","while True:\n","    ret, bgr_image = video_capture.read()\n","    \n","    if ret == False:\n","        break\n","    \n","    frame_time =  video_capture.get(cv2.CAP_PROP_POS_MSEC)\n","    if frame_time > 25000:\n","        break\n","    \n","    #if tot_frames > 1000:\n","    #    break\n","    \n","    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n","    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n","    face_locations = face_recognition.face_locations(rgb_image, model=\"cnn\")\n","    tot_frames += 1\n","    \n","    maxh, maxw = rgb_image.shape[:2]\n","    print(\"maxh: {0}, maxw: {1}\".format(maxh,maxw))\n","\n","    for top, right, bottom, left in face_locations:\n","        \n","        face_index += 1\n","        \n","        face_dict = {}\n","        \n","        #face_dict[\"rgb_image\"] = rgb_image\n","        face_dict[\"bb\"] = (top, right, bottom, left)\n","        face_dict[\"face_index\"] = face_index\n","        face_dict[\"frame_number\"] = tot_frames\n","        \n","        \n","        \n","        encodings = face_recognition.face_encodings(rgb_image, [(top, right, bottom, left)])\n","        \n","        face_dict[\"encodings\"] = encodings[0]\n","        \n","        x_off, y_off = emotion_offsets\n","        #n_left, n_right, n_top, n_bottom = apply_offsets((left,top,right-left,bottom-top), emotion_offsets)\n","        n_left, n_right, n_top, n_bottom = (max(left - x_off,0), min(right + x_off, maxw), max(top - y_off,0), min(bottom + y_off, maxh))\n","        print(\"top: {0}\\tright: {1}\\tbottom: {2}\\tleft: {3}\".format(top, right, bottom, left))\n","        print(\"n_top: {0}\\tn_right: {1}\\tn_bottom: {2}\\tn_left: {3}\".format(n_top, n_right, n_bottom, n_left))\n","        \n","        gray_face = gray_image[n_top:n_bottom, n_left:n_right]\n","        try:\n","            gray_face = cv2.resize(gray_face, (emotion_target_size))\n","        except:\n","            continue\n","\n","        gray_face = preprocess_input(gray_face, True)\n","        gray_face = np.expand_dims(gray_face, 0)\n","        gray_face = np.expand_dims(gray_face, -1)\n","        emotion_prediction = emotion_classifier.predict(gray_face)\n","        emotion_probability = np.max(emotion_prediction)\n","        emotion_label_arg = np.argmax(emotion_prediction)\n","        emotion_text = emotion_labels[emotion_label_arg]\n","        face_dict[\"emotion_text\"] = emotion_text\n","        \n","        x_off, y_off = gender_offsets\n","        n_left, n_right, n_top, n_bottom = (max(left - x_off,0), min(right + x_off, maxw), max(top - y_off,0), min(bottom + y_off, maxh))\n","        rgb_face = rgb_image[n_top:n_bottom, n_left:n_right]\n","        try:\n","            rgb_face = cv2.resize(rgb_face, (gender_target_size))\n","        except:\n","            continue\n","\n","        rgb_face = preprocess_input(rgb_face, True)\n","        rgb_face = np.expand_dims(rgb_face, 0)\n","        gender_prediction = gender_classifier.predict(rgb_face)\n","        gender_label_arg = np.argmax(gender_prediction)\n","        gender_text = gender_labels[gender_label_arg]\n","        face_dict[\"gender_text\"] = gender_text\n","\n","        face_dict[\"frame_time\"] = frame_time\n","        \n","        face_data.append(face_dict)\n","        print(\"face_data len: %d\"%len(face_data))\n","        if tot_frames % 1 == 0:\n","            print(\"face_index: \",face_index)\n","            print(\"frame_time: \", frame_time)\n","            print(\"emotion_text: \", emotion_text)\n","            print(\"frame_number: \", tot_frames)\n","            print(\"gender_text: \", gender_text)\n","            rgb_image_copy = rgb_image.copy()\n","        \n","            cv2.rectangle(rgb_image_copy, (left, top), (right, bottom), (255, 0, 0), 2)\n","            cropped_face = rgb_image_copy[top:bottom, left:right,:]\n","            print(cropped_face.shape)\n","            plt.axis(\"off\")\n","            plt.imshow(cropped_face)\n","            plt.show()\n","        \n","        '''\n","        emotion_window.append(emotion_text)\n","\n","        if len(emotion_window) > frame_window:\n","            emotion_window.pop(0)\n","        try:\n","            emotion_mode = mode(emotion_window)\n","        except:\n","            continue\n","\n","       \n","        if emotion_text == 'angry':\n","            color = emotion_probability * np.asarray((255, 0, 0))\n","        elif emotion_text == 'sad':\n","            color = emotion_probability * np.asarray((0, 0, 255))\n","        elif emotion_text == 'happy':\n","            color = emotion_probability * np.asarray((255, 255, 0))\n","        elif emotion_text == 'surprise':\n","            color %matplotlib inline= emotion_probability * np.asarray((0, 255, 255))\n","        else:\n","            color = emotion_probability * np.asarray((0, 255, 0))\n","\n","        color = color.astype(int)\n","        color = color.tolist()\n","\n","        draw_bounding_box(face_coordinates, rgb_image, color)\n","        draw_text(face_coordinates, rgb_image, emotion_mode,\n","                  color, 0, -45, 1, 1)\n","        '''\n","        \n","        \n","        \n","        \n","    #bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n","    #cv2.imshow('window_frame', bgr_image)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","        \n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGnDcM1Y6VQ6","colab_type":"code","colab":{}},"source":["print(tot_frames)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Tzllyow9h41","colab_type":"code","colab":{}},"source":["print(index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsFPHCAj6Yo6","colab_type":"code","colab":{}},"source":["print(len(face_data))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8sz7Il9RLPB","colab_type":"code","colab":{}},"source":["f = open(\"face_data\", \"wb\")\n","f.write(pickle.dumps(face_data))\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HaAKOf-RKAf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFazQYlyvZFW","colab_type":"code","colab":{}},"source":["video_capture.release()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZmybbXN7jm2","colab_type":"code","colab":{}},"source":["face_data = pickle.loads(open('face_data', \"rb\").read())\n","face_data = np.array(face_data)\n","encodings = [d[\"encodings\"] for d in face_data]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHszoK_I9QlL","colab_type":"code","colab":{}},"source":["print(len(encodings))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUPyk5Th8q1Q","colab_type":"code","colab":{}},"source":["from sklearn.cluster import DBSCAN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_2ofJqM8N4K","colab_type":"code","colab":{}},"source":["clt = DBSCAN(metric=\"euclidean\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aF-53xTE8631","colab_type":"code","colab":{}},"source":["clt.fit(encodings)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbrjGlTZ9rMH","colab_type":"code","colab":{}},"source":["print(clt.labels_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDZTTu8H87e_","colab_type":"code","colab":{}},"source":["labelIDs = np.unique(clt.labels_)\n","print(labelIDs)\n","numUniqueFaces = len(np.where(labelIDs > -1)[0])\n","print(\"[INFO] # unique faces: {}\".format(numUniqueFaces))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3z79vmTisKH","colab_type":"code","colab":{}},"source":["face_data_classified = [[] for i in range(len(labelIDs))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OC1nngNzjDam","colab_type":"code","colab":{}},"source":["for idx, label in enumerate(clt.labels_):\n","    face_data_classified[label].append(face_data[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHjwxQDsTuip","colab_type":"code","colab":{}},"source":["id_list = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1JiONDSTpov","colab_type":"code","colab":{}},"source":["for idx in range(len(labelIDs)):\n","    if len(face_data_classified[idx]) >= 25:\n","        id_list.append(idx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"29m6MBAMWQvE","colab_type":"code","colab":{}},"source":["id_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEW7oD1AwWeL","colab_type":"code","colab":{}},"source":["frame_window = 20\n","emotion_window = []\n","#gender_window = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqMlmg8jukQ9","colab_type":"code","colab":{}},"source":["for idx, label in enumerate(clt.labels_):\n","    if label in id_list:\n","        face_data_classified[label].append(face_data[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WP1x4Hp8L_00","colab_type":"code","colab":{}},"source":["for idx in range(len(labelIDs)):\n","    print(len(face_data_classified[idx]))"],"execution_count":0,"outputs":[]}]}